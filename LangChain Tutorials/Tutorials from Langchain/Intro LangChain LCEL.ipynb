{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to LangChain LCEL\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/10/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6641e5aab937d1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "**In this project:**  I get started using LCEL.  \n",
    "\n",
    "<p></p>\n",
    "<b class=\"alert alert-block alert-info\" style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAi key.\n",
    "</b>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4802bbafc85fa8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Project map:\n",
    "- API Key\n",
    "- Getting started\n",
    "    - Basic example **```chain = prompt | model | output_parser```**\n",
    "        - Prompt (Human Message) **```prompt_value = prompt.invoke({\"topic\": \"ice cream\"})```**\n",
    "        - Model (AIMessage) **```message = model.invoke(prompt_value)```**\n",
    "                - ChatModel Output\n",
    "                - LLM Output\n",
    "        - Output parser (takes AIMessage or any string and converts it) **```output_parser.invoke(message)```**\n",
    "        - Entire Pipeline **```(prompt | model | output_parser).invoke({\"topic\": \"ice cream\"})```**\n",
    "- RAG Search **```chain = setup_and_retrieval | prompt | model | output_parser```**\n",
    "    - Example\n",
    "    - Example Explanation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea627a8a1b77809c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### API Keys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b9291b67ba80e3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:30.442722200Z",
     "start_time": "2024-01-19T00:30:30.432349500Z"
    }
   },
   "id": "5227e6746c0400ca",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8a87af4746cb80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic example: \n",
    "prompt + model + output parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c35d5c7954c11b8d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser #https://python.langchain.com/docs/modules/model_io/output_parsers/\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:30.856185700Z",
     "start_time": "2024-01-19T00:30:30.437739600Z"
    }
   },
   "id": "45d95e11e0d6b3a6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always melt under pressure!\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:32.685296700Z",
     "start_time": "2024-01-19T00:30:30.856185700Z"
    }
   },
   "id": "6bb98f8e57b0c18d",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    " #### Prompt (Human Message)\n",
    "```prompt``` is a ```BasePromptTemplate```, which means it takes in a dictionary of template variables and produces a ```PromptValue```.   \n",
    "\n",
    "A ```PromptValue``` is a wrapper around a completed ```prompt``` that can be passed to either an ```LLM``` (which takes a string as input) or ```ChatModel``` (which takes a sequence of messages as input). \n",
    "It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ebcb086543c1239"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:32.688600800Z",
     "start_time": "2024-01-19T00:30:32.682989100Z"
    }
   },
   "id": "9a70eae173b0c034",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='tell me a short joke about ice cream')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:32.700003100Z",
     "start_time": "2024-01-19T00:30:32.686607500Z"
    }
   },
   "id": "796cfe4b935ded64",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Human: tell me a short joke about ice cream'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:32.838873200Z",
     "start_time": "2024-01-19T00:30:32.696015100Z"
    }
   },
   "id": "eb8112bb32b94303",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model (AIMessage)\n",
    "The ```PromptValue``` is then passed to ```model```. In this case our ```model``` is a ```ChatModel```, meaning it will output a ```BaseMessage```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71dc01ecf5c75ff6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ChatModel Output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83ba2ac8e756ec2f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when they're getting ready!\")"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:34.319640200Z",
     "start_time": "2024-01-19T00:30:32.707000200Z"
    }
   },
   "id": "b869174bc6381760",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### LLM Output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24b7f7a5b52425e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\nRobot: Why did the ice cream go to therapy? Because it was feeling a little melted.'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm.invoke(prompt_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:35.239856500Z",
     "start_time": "2024-01-19T00:30:34.316650300Z"
    }
   },
   "id": "43a81fc85a2c8ea8",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output parser (takes AIMessage or any string and converts it)\n",
    "And lastly we pass our ```model``` output to the ```output_parser```, which is a ```BaseOutputParser``` meaning it takes either a ```string``` or a ```BaseMessage``` as input. \n",
    "The ```StrOutputParser``` specifically simple converts any input into a ```string```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa27c59ad072de1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when they're getting ready!\""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:35.240853100Z",
     "start_time": "2024-01-19T00:30:35.231804800Z"
    }
   },
   "id": "35993835db862d01",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Entire Pipeline\n",
    "**```(prompt | model | output_parser).invoke({\"topic\": \"ice cream\"})```**\n",
    "\n",
    "To follow the steps along:\n",
    "1. We pass in user input on the desired topic as ```{\"topic\": \"ice cream\"}```\n",
    "2. The ```prompt``` component takes the user input, which is then used to construct a ```PromptValue``` after using the ```topic``` to construct the prompt.\n",
    "3. The ```model``` component takes the generated ```prompt```, and passes into the ```OpenAI LLM model``` for evaluation. The generated output from the model is a ```ChatMessage``` object.\n",
    "4. Finally, the ```output_parser``` component takes in a ```ChatMessage```, and transforms this into a Python ```string```, which is returned from the ```invoke``` method.\n",
    "\n",
    "Note that if you’re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82ec572ae2dad8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring a meltdown!\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "(prompt | model | output_parser).invoke({\"topic\": \"ice cream\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:36.292618900Z",
     "start_time": "2024-01-19T00:30:35.234394500Z"
    }
   },
   "id": "be56025ac9c21bc3",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84305ebd3bcf7b9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example\n",
    "For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.\n",
    "\n",
    "To be error free this example requires:\n",
    "```\n",
    "pydantic==1.10.8 \n",
    "docarray==0.40.0 \n",
    "langchain docarray tiktoken\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3742569215a94c18"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:36.304566600Z",
     "start_time": "2024-01-19T00:30:36.293615400Z"
    }
   },
   "id": "a639ef9a297bde52",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embedding (OpenAI) - Vectorstore"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49e1aefde2fce6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts( # Text Doc in memory \n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"], # Doc-text\n",
    "    embedding=OpenAIEmbeddings(), # embedding\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:37.301678400Z",
     "start_time": "2024-01-19T00:30:36.302573900Z"
    }
   },
   "id": "e5bc61ab40969754",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:37.302679300Z",
     "start_time": "2024-01-19T00:30:37.300748400Z"
    }
   },
   "id": "28a672a37c4a3223",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:37.642404600Z",
     "start_time": "2024-01-19T00:30:37.302679300Z"
    }
   },
   "id": "d7b4d8a7336a6dfe",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### RunnableParallel object"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbaa16a5a73483b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:30:37.644402600Z",
     "start_time": "2024-01-19T00:30:37.640889Z"
    }
   },
   "id": "83aeebc5ab43f82d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Harrison worked at Kensho.'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "chain.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:51:24.633442200Z",
     "start_time": "2024-01-19T00:51:22.502426600Z"
    }
   },
   "id": "3d7d9e27789f71a2",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example Explanation\n",
    "\n",
    "To explain this, we first can see that the prompt template above takes in ``context`` and ```question``` as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context. \n",
    "**```setup_and_retrieval = RunnableParallel( {\"context\": retriever, \"question\": RunnablePassthrough()} )```**\n",
    "\n",
    "As a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "650f582508cbff3c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='harrison worked at kensho'),\n Document(page_content='bears like to eat honey')]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:43:04.609744700Z",
     "start_time": "2024-01-19T00:43:03.346509300Z"
    }
   },
   "id": "ba306c4795f792b5",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  context: VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x000002357F3A55D0>),\n  question: RunnablePassthrough()\n}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_and_retrieval"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T00:46:10.987736400Z",
     "start_time": "2024-01-19T00:46:10.916198700Z"
    }
   },
   "id": "3e686e882e771412",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the flow being:\n",
    "\n",
    "1. The first steps create a ```RunnableParallel``` object with two entries. \n",
    "- The first entry, ```context``` will include the document results fetched by the ```retriever```. \n",
    "- The second entry, question will contain the user’s original ```question```. To pass on the question, we use ```RunnablePassthrough``` to copy this entry.\n",
    "2. Feed the dictionary from the step above to the ```prompt``` component. It then takes the user input which is ```question``` as well as the ```retrieved document``` which is ```context``` to construct a prompt and output a ```PromptValue```.\n",
    "3. The ```model``` component takes the generated prompt, and passes into the ```OpenAI LLM``` model for evaluation. The generated output from the model is a ```ChatMessage``` object.\n",
    "4. Finally, the ```output_parser``` component takes in a ```ChatMessage```, and transforms this into a Python ```string```, which is returned from the invoke method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392450717f0219b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
