{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model IO\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "---\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/22/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/)\n",
    "\n",
    "<br>\n",
    "\n",
    "--- "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56c667153f33f793"
  },
  {
   "cell_type": "markdown",
   "source": [
    " \n",
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models.  \n",
    "**In this project:**  \n",
    "I explore the concept Model i/o, the core element of any language model application is...the model.   \n",
    "LangChain gives you the building blocks to interface with any language model.\n",
    "<br>\n",
    "<img src=\"pic/model_io.jpg\" width=\"1500\">\n",
    "<br>\n",
    "\n",
    "[Conceptual Guide](https://python.langchain.com/docs/modules/model_io/concepts)  \n",
    "A conceptual explanation of messages, prompts, LLMs vs ChatModels, and output parsers. You should read this before getting started.  \n",
    "[Quickstart](https://python.langchain.com/docs/modules/model_io/quick_start)  \n",
    "Covers the basics of getting started working with different types of models. You should walk through [this section](https://python.langchain.com/docs/modules/model_io/output_parsers/) if you want to get an overview of the functionality.  \n",
    "[Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)  \n",
    "This section deep dives into the different types of prompt templates and how to use them.  \n",
    "[LLMs](https://python.langchain.com/docs/modules/model_io/llms/)  \n",
    "This section covers functionality related to the LLM class. This is a type of model that takes a text string as input and returns a text string.  \n",
    "[ChatModels](https://python.langchain.com/docs/modules/model_io/chat/)  \n",
    "This section covers functionality related to the ChatModel class. This is a type of model that takes a list of messages as input and returns a message.  \n",
    "[Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)  \n",
    "Output parsers are responsible for transforming the output of LLMs and ChatModels into more structured data. This section covers the different types of output parsers.  \n",
    "\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAI key.\n",
    "</b>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23d747506242aa53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Project Map  \n",
    "- [API Keys](#api-keys)  \n",
    "- [Quickstart](#quickstart)\n",
    "    - [Models](#models)\n",
    "    - [Prompt Templates](#prompt-templates)\n",
    "    - [Output parsers](#output-parsers)\n",
    "    - [Composing with LCEL](#composing-with-lcel)\n",
    "- [Concepts](#concepts)\n",
    "     - [Models (concept)](#models-concept)\n",
    "     - [Messages (concept)](#messages-concept)\n",
    "     - [Prompts (concept)](#prompts-concept)\n",
    "     - [Output Parsers (concept)](#output-parsers-concept)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d812a5237c2ffca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### API Keys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aadd2d6d518060f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T23:44:13.024361500Z",
     "start_time": "2024-01-22T23:44:13.017854800Z"
    }
   },
   "id": "9f0bea1dbc415b2",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0f050905d2169f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Quickstart\n",
    "\n",
    "The quick start will cover the basics of working with language models. It will introduce the two different types of models - LLMs and ChatModels. It will then cover how to use PromptTemplates to format the inputs to these models, and how to use Output Parsers to work with the outputs. For a deeper conceptual guide into these topics - please see this [documentation](https://python.langchain.com/docs/modules/model_io/concepts).\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cef7b67794ba6231"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models\n",
    "For this getting started guide, I will provide the options of using OpenAI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d33c7af48e154e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### OpenAI\n",
    "\n",
    "Both ```llm``` and ```chat_model``` are objects that represent configuration for a particular model. You can initialize them with parameters like ```temperature``` and others, and pass them around. \n",
    "**The main difference between them is their input and output schemas.**\n",
    "- The ```LLM``` objects take ```string``` as ```input``` and ```output string```. \n",
    "- The ```ChatModel``` objects take a ```list of messages``` as ```input``` and ```output a message```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ea980d07a67224"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T23:49:47.150515200Z",
     "start_time": "2024-01-22T23:49:46.563283100Z"
    }
   },
   "id": "f834ff5450d0114f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"If my wife says that I am a lucky individual, am I a lucky individual?\"\n",
    "messages = [HumanMessage(content=text)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T23:57:15.407610200Z",
     "start_time": "2024-01-22T23:57:15.401636100Z"
    }
   },
   "id": "8ac207463c84935d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'?\\n\\nIt is subjective and ultimately up to personal interpretation. Some may believe that being called \"lucky\" by one\\'s spouse is a sign of good fortune, while others may not put much weight on the opinion of one person. Ultimately, the label of \"lucky\" is based on a person\\'s own experiences and perspective.'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T23:57:18.548197100Z",
     "start_time": "2024-01-22T23:57:17.518909Z"
    }
   },
   "id": "26e050bcb71d5694",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Yes, if your wife says that you are a lucky individual, it means that she believes you have good fortune or that positive things continuously happen to you.')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T23:57:20.693278800Z",
     "start_time": "2024-01-22T23:57:18.886563600Z"
    }
   },
   "id": "ea38ae14d8c9c4de",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83df04a1c239ee8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product without worrying about giving the model instructions.\n",
    "\n",
    "PromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5acbc4014b0a7c44"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'What is a good name for a company that makes colorful socks?'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T00:02:35.348043600Z",
     "start_time": "2024-01-23T00:02:35.315757200Z"
    }
   },
   "id": "5d145831a9f37962",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the [section on prompts](https://python.langchain.com/docs/modules/model_io/prompts) for more detail.\n",
    "\n",
    "```PromptTemplates``` can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a ChatPromptTemplate is a list of ```ChatMessageTemplates```. Each ```ChatMessageTemplate``` contains instructions for how to format that ```ChatMessage``` - its role, and then also its content. Let's take a look at this below:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c36893ef9af070c4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n HumanMessage(content='I love programming.')]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T00:46:39.180234200Z",
     "start_time": "2024-01-23T00:46:39.163400700Z"
    }
   },
   "id": "d1cdfd7cedf8225a",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20bba279160006c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output parsers\n",
    "\n",
    "```OutputParsers``` convert the raw output of a language model into a format that can be used downstream. There are a few main types of ```OutputParser```s, including:\n",
    "\n",
    "Convert text from ```LLM``` into structured information (e.g. JSON)\n",
    "Convert a ```ChatMessage``` into just a string\n",
    "Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
    "For full information on this, see the [section on output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "In this getting started guide, we use a simple one that parses a list of comma separated values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb5c60a9a1efad7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['hi', 'bye']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T00:54:54.547747200Z",
     "start_time": "2024-01-23T00:54:54.481392400Z"
    }
   },
   "id": "f26d26dbfa9ced78",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9973ca8fa7aba9b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composing with LCEL\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2105e16d574eab9a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['red', 'blue', 'green', 'yellow', 'purple']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T00:56:11.813839400Z",
     "start_time": "2024-01-23T00:56:09.930641Z"
    }
   },
   "id": "336d7919ca154ec9",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e09e0826c4380dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Concept\n",
    "\n",
    "[Concept](https://python.langchain.com/docs/modules/model_io/concepts)\n",
    "The core element of any language model application is...the model. LangChain gives you the building\n",
    "blocks to interface with any language model. Everything in this section is about making it easier to work\n",
    "with models. This largely involves a clear interface for what a model is, helper utils for constructing\n",
    "inputs to models, and helper utils for working with the outputs of models\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a2b1ab02b8a19f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models (concept)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e58d903fe4c95162"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LLM**s \n",
    "LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string\n",
    "prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM.\n",
    "\n",
    "**Chat Models**\n",
    "Chat models are often backed by LLMs but tuned specifically for having conversations. Crucially,\n",
    "their provider APIs use a different interface than pure text completion models. Instead of a\n",
    "single string, they take a list of chat messages as input and they return an AI message as output.\n",
    "See the section below for more details on what exactly a message consists of. GPT-4 and\n",
    "Anthropic's Claude-2 are both implemented as chat models.\n",
    "\n",
    "Considerations\n",
    "These two API types have pretty different input and output schemas. This means that best way\n",
    "to interact with them may be quite different. Although LangChain makes it possible to treat\n",
    "them interchangeably, that doesn't mean you should. In particular, the prompting strategies for\n",
    "LLMs vs ChatModels may be quite different. This means that you will want to make sure the\n",
    "prompt you are using is designed for the model type you are working with.\n",
    "\n",
    "Additionally, not all models are the same. Different models have different prompting strategies\n",
    "that work best for them. For example, Anthropic's models work best with XML while OpenAI's\n",
    "work best with JSON. This means that the prompt you use for one model may not transfer to\n",
    "other ones. LangChain provides a lot of default prompts, however these are not guaranteed to\n",
    "work well with the model are you using. Historically speaking, most prompts work well with\n",
    "OpenAI but are not heavily tested on other models. This is something we are working to address,\n",
    "but it is something you should keep in mind."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f819f0e0f2a43d8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975a3b8010f767bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Messages (concept)\n",
    "\n",
    "ChatModels take a list of messages as input and return a message. There are a few different types of\n",
    "messages. All messages have a role and a content property.\n",
    "The role describes WHO is saying the message. LangChain has different message classes for different\n",
    "roles.\n",
    "The content property describes the content of the message.\n",
    "\n",
    "This can be a few different things:\n",
    "- A string (most models are this way)\n",
    "- A List of dictionaries (this is used for multi-modal input, where the dictionary contains information about that input type and that input location)\n",
    "\n",
    "In addition, messages have an additional_kwargs property. This is where additional information about\n",
    "messages can be passed. This is largely used for input parameters that are provider specific and not\n",
    "general. The best known example of this is function_call from OpenAI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f9ce41a26a1baf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**HumanMessage**\n",
    "This represents a message from the user. Generally consists only of content.\n",
    "\n",
    "**AIMessage**\n",
    "This represents a message from the model. This may have additional_kwargs in it - for\n",
    "example functional_call if using OpenAI Function calling.\n",
    "\n",
    "**SystemMessage**\n",
    "This represents a system message. Only some models support this. This tells the model how to\n",
    "behave. This generally only consists of content.\n",
    "\n",
    "**FunctionMessage**\n",
    "This represents the result of a function call. In addition to role and content, this message has\n",
    "a name parameter which conveys the name of the function that was called to produce this\n",
    "result.\n",
    "\n",
    "**ToolMessage**\n",
    "This represents the result of a tool call. This is distinct from a FunctionMessage in order to match\n",
    "OpenAI's function and tool message types. In addition to role and content, this message has\n",
    "a tool_call_id parameter which conveys the id of the call to the tool that was called to produce\n",
    "this result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3f61e9e264b6fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d49d1d35197f931"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompts (concept)\n",
    "The inputs to language models are often called prompts. Oftentimes, the user input from your app is not\n",
    "the direct input to the model. Rather, their input is transformed in some way to produce the string or list\n",
    "of messages that does go into the model. The objects that take user input and transform it into the final\n",
    "string or messages are known as \"Prompt Templates\". LangChain provides several abstractions to make\n",
    "working with prompts easier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de2d5d335cba0f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PromptValue**\n",
    "ChatModels and LLMs take different input types. PromptValue is a class designed to be\n",
    "interoperable between the two. It exposes a method to be cast to a string (to work with LLMs)\n",
    "and another to be cast to a list of messages (to work with ChatModels).\n",
    "\n",
    "**PromptTemplate**\n",
    "This is an example of a prompt template. This consists of a template string. This string is then\n",
    "formatted with user inputs to produce a final string.\n",
    "\n",
    "**MessagePromptTemplate**\n",
    "This is an example of a prompt template. This consists of a template message - meaning a\n",
    "specific role and a PromptTemplate. This PromptTemplate is then formatted with user inputs to\n",
    "\n",
    "\n",
    "**HumanMessagePromptTemplate**\n",
    "This is MessagePromptTemplate that produces a HumanMessage.\n",
    "\n",
    "**AIMessagePromptTemplate**\n",
    "This is MessagePromptTemplate that produces an AIMessage.\n",
    "\n",
    "**SystemMessagePromptTemplate**\n",
    "This is MessagePromptTemplate that produces a SystemMessage.\n",
    "\n",
    "**MessagesPlaceholder**\n",
    "Oftentimes inputs to prompts can be a list of messages. This is when you would use a\n",
    "MessagesPlaceholder. These objects are parameterized by a variable_name argument. The input\n",
    "with the same value as this variable_name value should be a list of messages.\n",
    "\n",
    "**ChatPromptTemplate**\n",
    "This is an example of a prompt template. This consists of a list of MessagePromptTemplates or\n",
    "MessagePlaceholders. These are then formatted with user inputs to produce a final list of\n",
    "messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e44c36a587440fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32163149c9029f73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output Parsers (concept)\n",
    "\n",
    "The output of models are either strings or a message. Oftentimes, the string or messages contains\n",
    "information formatted in a specific format to be used downstream (e.g. a comma separated list, or JSON\n",
    "blob). Output parsers are responsible for taking in the output of a model and transforming it into a more\n",
    "usable form. These generally work on the content of the output message, but occasionally work on\n",
    "values in the additional_kwargs field."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6256f9695c58e390"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**StrOutputParser**\n",
    "This is a simple output parser that just converts the output of a language model (LLM or\n",
    "ChatModel) into a string. If the model is an LLM (and therefore outputs a string) it just passes\n",
    "that string through. If the output is a ChatModel (and therefore outputs a message) it passes\n",
    "through the .content attribute of the message.\n",
    "\n",
    "**OpenAI Functions Parsers**\n",
    "There are a few parsers dedicated to working with OpenAI function calling. They take the output\n",
    "of the function_call and arguments parameters (which are inside additional_kwargs) and work\n",
    "with those, largely ignoring content.\n",
    "\n",
    "**Agent Output Parsers**\n",
    "[Agents](https://python.langchain.com/docs/modules/agents) are systems that use language models to determine what steps to take. The output of a\n",
    "language model therefore needs to be parsed into some schema that can represent what actions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b89e92ef15b8a01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79e0b0412a29b897"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
