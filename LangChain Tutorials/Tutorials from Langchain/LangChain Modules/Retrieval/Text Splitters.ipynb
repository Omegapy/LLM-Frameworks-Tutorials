{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c667153f33f793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Text Splitters\n",
    "\n",
    "---\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/23/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/)\n",
    "\n",
    "<br>\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d747506242aa53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " \n",
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models.  \n",
    "**In this project:** This project is a series of LangChain text tplitters for LLMs tutorials on Jupyter Notebook.  \n",
    "The tutorials are a series LangChain Python code examples from the https://python.langchain.com/ website\n",
    " \n",
    "Specifically from the section [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "1.\tSplit the text up into small, semantically meaningful chunks (often sentences).\n",
    "2.\tStart combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3.\tOnce you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "1.\tHow the text is split\n",
    "2.\tHow the chunk size is measured\n",
    "\n",
    "**Types of Text Splitters**\n",
    "LangChain offers many different types of text splitters. Below is a table listing all of them, along with a few characteristics:\n",
    "\n",
    "**Name**: Name of the text splitter\n",
    "\n",
    "**Splits On**: How this text splitter splits text\n",
    "\n",
    "**Adds Metadata**: Whether or not this text splitter adds metadata about where each chunk came from.\n",
    "\n",
    "**Description**: Description of the splitter, including recommendation on when to use it.\n",
    "\n",
    "<p></p>\n",
    "<table><thead><tr><th>Name</th><th>Splits On</th><th>Adds Metadata</th><th>Description</th></tr></thead><tbody><tr><td>Recursive</td><td>A list of user defined characters</td><td></td><td>Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.</td></tr><tr><td>HTML</td><td>HTML specific characters</td><td>✅</td><td>Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)</td></tr><tr><td>Markdown</td><td>Markdown specific characters</td><td>✅</td><td>Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)</td></tr><tr><td>Code</td><td>Code (Python, JS) specific characters</td><td></td><td>Splits text based on characters specific to coding languages. 15 different languages are available to choose from.</td></tr><tr><td>Token</td><td>Tokens</td><td></td><td>Splits text on tokens. There exist a few different ways to measure tokens.</td></tr><tr><td>Character</td><td>A user defined character</td><td></td><td>Splits text based on a user defined character. One of the simpler methods.</td></tr><tr><td>[Experimental] Semantic Chunker</td><td>Sentences</td><td></td><td>First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from <a href=\"https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Greg Kamradt</a></td></tr></tbody></table>\n",
    "\n",
    "**Evaluate text splitters**\n",
    "You can evaluate text splitters with the [Chunkviz utility](https://chunkviz.up.railway.app/) created by Greg Kamradt. ```Chunkviz``` is a great tool for visualizing how your text splitter is working. It will show you how your text is being split up and help in tuning up the splitting parameters.\n",
    "\n",
    "**Other Document Transforms**\n",
    "Text splitting is only one example of transformations that you may want to do on documents before passing them to an LLM. Head to Integrations for documentation on built-in document transformer [integrations](https://python.langchain.com/docs/integrations/document_transformers/) with 3rd-party tools.\n",
    "\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAI key.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d812a5237c2ffca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Project Map  \n",
    "- [API Keys](#api-keys)  \n",
    "- [HTMLHeaderTextSplitter](#htmlheadertextsplitter)\n",
    "    - [Base Example](#base-example-htmlheadertextsplitter)\n",
    "    - [Limitations](#limitations)\n",
    "- [Split by character](#split-by-character)\n",
    "- [Split code](#split-code)\n",
    "    - [Python](#python)\n",
    "    - [JS](#js)\n",
    "    - [TS](#ts)\n",
    "    - [Markdown](#markdown)\n",
    "    - [Latex](#latex)\n",
    "    - [HTML](#html)\n",
    "    - [Solidity](#solidity)\n",
    "    - [c](#c)\n",
    "- [MarkdownHeaderTextSplitter](#markdownheadertextsplitter)\n",
    "- [Recursively split by character](#recursively-split-by-character)\n",
    "- [Semantic Chunking](#semantic-chunking)\n",
    "- [Split by tokens](#split-by-tokens)\n",
    "    - [tiktoken](#tiktoken)\n",
    "    - [spaCy](#spacy)\n",
    "    - [SentenceTransformers](#sentencetransformers)\n",
    "    - [NTLK](#nltk)\n",
    "    - [Hugging Face tokenizer](#hugging-face-tokenizer)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadd2d6d518060f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0bea1dbc415b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:03.489326400Z",
     "start_time": "2024-02-04T15:32:03.087746400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from openai.types.beta.threads.messages import message_file\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc6fd5480d3b4d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## HTMLHeaderTextSplitter\n",
    "\n",
    "Similar in concept to the\n",
    "\n",
    "`MarkdownHeaderTextSplitter`, the `HTMLHeaderTextSplitter` is a “structure-aware” chunker that splits text at the element level and adds metadata for each header “relevant” to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22eb33f203c48d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Base Example (HTMLHeaderTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df724bb5216a7ee7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1) With an HTML string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625617dc3f2a9137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:03.596935900Z",
     "start_time": "2024-02-04T15:32:03.463751800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Foo'),\n",
       " Document(page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some intro text about Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}),\n",
       " Document(page_content='Some text about the first subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}),\n",
       " Document(page_content='Some text about the second subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}),\n",
       " Document(page_content='Baz', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some text about Baz', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}),\n",
       " Document(page_content='Some concluding text about Foo', metadata={'Header 1': 'Foo'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed371c18288343ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2) Pipelined to another splitter, with html loaded from a web URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd12b2af7b6ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:05.347485300Z",
     "start_time": "2024-02-04T15:32:03.529038700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We see that Gödel first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (“This sentence is false”) and Berry’s paradox (“The least number not defined by an expression consisting of just fourteen English words”). Gödel then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='means that arithmetic truth and arithmetic provability are not co-extensive — whence the First Incompleteness Theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='This account of Gödel’s discovery was told to Hao Wang very much after the fact; but in Gödel’s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See Gödel 2003a and Gödel 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by Gödel by 1931. But he neither publicized nor published the result; the biases logicians', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to Gödel’s publication of that theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='We now describe the proof of the two theorems, formulating Gödel’s results in Peano arithmetic. Gödel himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following Gödel’s notation.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "splits[80:85]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f050905d2169f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85ba97eaca8f1c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Limitations\n",
    "There can be quite a bit of structural variation from one HTML document to another, and while HTMLHeaderTextSplitter will attempt to attach all “relevant” headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes “above” associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged “h1”, is in a distinct subtree from the text elements that we’d expect it to be “above”—so we can observe that the “h1” element and its associated text do not show up in the chunk metadata (but, where applicable, we do see “h2” and its associated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6504b6624ea19897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.000273100Z",
     "start_time": "2024-02-04T15:32:05.346485400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No two El Niño winters are the same, but many have temperature and precipitation trends in common.  \n",
      "Average conditions during an El Niño winter across the continental US.  \n",
      "One of the major reasons is the position of the jet stream, which often shifts south during an El Niño winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  \n",
      "Because the jet stream is essentially a river of air that storms flow through, they c\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "print(html_header_splits[1].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c2869b05122a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa749bf60ea6ce7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Split by character\n",
    "\n",
    "This is the simplest method. This splits based on characters (by default “”) and measure chunk length by number of characters.\n",
    "\n",
    "1. How the text is split: by single character.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4e689bf5eb7b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.004762300Z",
     "start_time": "2024-02-04T15:32:05.999272800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bec79ff9856277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.018766200Z",
     "start_time": "2024-02-04T15:32:06.003762200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8772b3014c36cac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.100325Z",
     "start_time": "2024-02-04T15:32:06.014257100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87764ed0198e2257",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here’s an example of passing metadata along with the documents, notice that it is split along with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637de9c109ebc944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.103325800Z",
     "start_time": "2024-02-04T15:32:06.023446500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "documents = text_splitter.create_documents(\n",
    "    [state_of_the_union, state_of_the_union], metadatas=metadatas\n",
    ")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5325476a91b8774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.104324700Z",
     "start_time": "2024-02-04T15:32:06.032458800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(state_of_the_union)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bd00a333698c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33006b22ad5cca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Split code\n",
    "\n",
    "```CodeTextSplitter``` allows you to split your code with multiple languages supported. Import ```enum Language``` and specify the language.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f182dd82d4fbcb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.212274Z",
     "start_time": "2024-02-04T15:32:06.039585700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# Full list of supported languages\n",
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2dedc48d87db12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.214273900Z",
     "start_time": "2024-02-04T15:32:06.046603700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also see the separators used for a given language\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb70542702ba62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76fabc8ac6aa6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.216780600Z",
     "start_time": "2024-02-04T15:32:06.054654600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a777a411d67a6ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6c84235d2e86d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17129a3fd38c13de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.217779600Z",
     "start_time": "2024-02-04T15:32:06.062620600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'),\n",
       " Document(page_content='// Call the function\\nhelloWorld();')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JS_CODE = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "// Call the function\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "js_docs = js_splitter.create_documents([JS_CODE])\n",
    "js_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104f271effb9d7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766198f00361f964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.359037700Z",
     "start_time": "2024-02-04T15:32:06.069900400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='function helloWorld(): void {'),\n",
       " Document(page_content='console.log(\"Hello, World!\");\\n}'),\n",
       " Document(page_content='// Call the function\\nhelloWorld();')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS_CODE = \"\"\"\n",
    "function helloWorld(): void {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "// Call the function\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "ts_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.TS, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "ts_docs = ts_splitter.create_documents([TS_CODE])\n",
    "ts_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3099dbfe4bb31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef556ec80e8360a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef112b712528547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.361036600Z",
     "start_time": "2024-02-04T15:32:06.076404900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# 🦜️🔗 LangChain'),\n",
       " Document(page_content='⚡ Building applications with LLMs through composability ⚡'),\n",
       " Document(page_content='## Quick Install\\n\\n```bash'),\n",
       " Document(page_content=\"# Hopefully this code block isn't split\"),\n",
       " Document(page_content='pip install langchain'),\n",
       " Document(page_content='```'),\n",
       " Document(page_content='As an open-source project in a rapidly developing field, we'),\n",
       " Document(page_content='are extremely open to contributions.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_text = \"\"\"\n",
    "# 🦜️🔗 LangChain\n",
    "\n",
    "⚡ Building applications with LLMs through composability ⚡\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "# Hopefully this code block isn't split\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d3a63162aa11f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "479c9bc08363a292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.363037800Z",
     "start_time": "2024-02-04T15:32:06.084429400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32248\\214146929.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  latex_text = \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle'),\n",
       " Document(page_content='\\\\section{Introduction}'),\n",
       " Document(page_content='Large language models (LLMs) are a type of machine learning'),\n",
       " Document(page_content='model that can be trained on vast amounts of text data to'),\n",
       " Document(page_content='generate human-like language. In recent years, LLMs have'),\n",
       " Document(page_content='made significant advances in a variety of natural language'),\n",
       " Document(page_content='processing tasks, including language translation, text'),\n",
       " Document(page_content='generation, and sentiment analysis.'),\n",
       " Document(page_content='\\\\subsection{History of LLMs}'),\n",
       " Document(page_content='The earliest LLMs were developed in the 1980s and 1990s,'),\n",
       " Document(page_content='but they were limited by the amount of data that could be'),\n",
       " Document(page_content='processed and the computational power available at the'),\n",
       " Document(page_content='time. In the past decade, however, advances in hardware and'),\n",
       " Document(page_content='software have made it possible to train LLMs on massive'),\n",
       " Document(page_content='datasets, leading to significant improvements in'),\n",
       " Document(page_content='performance.'),\n",
       " Document(page_content='\\\\subsection{Applications of LLMs}'),\n",
       " Document(page_content='LLMs have many applications in industry, including'),\n",
       " Document(page_content='chatbots, content creation, and virtual assistants. They'),\n",
       " Document(page_content='can also be used in academia for research in linguistics,'),\n",
       " Document(page_content='psychology, and computational linguistics.'),\n",
       " Document(page_content='\\\\end{document}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_text = \"\"\"\n",
    "\\documentclass{article}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\section{Introduction}\n",
    "Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "\n",
    "\\subsection{History of LLMs}\n",
    "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "\n",
    "\\subsection{Applications of LLMs}\n",
    "LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "latex_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "latex_docs = latex_splitter.create_documents([latex_text])\n",
    "latex_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b818c00ffbc51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10d8e3398ec4354d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.364038400Z",
     "start_time": "2024-02-04T15:32:06.092584800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='<!DOCTYPE html>\\n<html>'),\n",
       " Document(page_content='<head>\\n        <title>🦜️🔗 LangChain</title>'),\n",
       " Document(page_content='<style>\\n            body {\\n                font-family: Aria'),\n",
       " Document(page_content='l, sans-serif;\\n            }\\n            h1 {'),\n",
       " Document(page_content='color: darkblue;\\n            }\\n        </style>\\n    </head'),\n",
       " Document(page_content='>'),\n",
       " Document(page_content='<body>'),\n",
       " Document(page_content='<div>\\n            <h1>🦜️🔗 LangChain</h1>'),\n",
       " Document(page_content='<p>⚡ Building applications with LLMs through composability ⚡'),\n",
       " Document(page_content='</p>\\n        </div>'),\n",
       " Document(page_content='<div>\\n            As an open-source project in a rapidly dev'),\n",
       " Document(page_content='eloping field, we are extremely open to contributions.'),\n",
       " Document(page_content='</div>\\n    </body>\\n</html>')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>🦜️🔗 LangChain</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "            }\n",
    "            h1 {\n",
    "                color: darkblue;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>🦜️🔗 LangChain</h1>\n",
    "            <p>⚡ Building applications with LLMs through composability ⚡</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "html_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.HTML, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "html_docs = html_splitter.create_documents([html_text])\n",
    "html_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9548cf0413f2654",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f720479ce3e49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Solidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8a2168ade00a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.416538300Z",
     "start_time": "2024-02-04T15:32:06.100325Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='pragma solidity ^0.8.20;'),\n",
       " Document(page_content='contract HelloWorld {\\n   function add(uint a, uint b) pure public returns(uint) {\\n       return a + b;\\n   }\\n}')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOL_CODE = \"\"\"\n",
    "pragma solidity ^0.8.20;\n",
    "contract HelloWorld {\n",
    "   function add(uint a, uint b) pure public returns(uint) {\n",
    "       return a + b;\n",
    "   }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sol_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.SOL, chunk_size=128, chunk_overlap=0\n",
    ")\n",
    "sol_docs = sol_splitter.create_documents([SOL_CODE])\n",
    "sol_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842da1db8c9221f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a43099bff803926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.430731900Z",
     "start_time": "2024-02-04T15:32:06.107621800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='using System;'),\n",
       " Document(page_content='class Program\\n{\\n    static void Main()\\n    {\\n        int age = 30; // Change the age value as needed'),\n",
       " Document(page_content='// Categorize the age without any console output\\n        if (age < 18)\\n        {\\n            // Age is under 18'),\n",
       " Document(page_content='}\\n        else if (age >= 18 && age < 65)\\n        {\\n            // Age is an adult\\n        }\\n        else\\n        {'),\n",
       " Document(page_content='// Age is a senior citizen\\n        }\\n    }\\n}')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_CODE = \"\"\"\n",
    "using System;\n",
    "class Program\n",
    "{\n",
    "    static void Main()\n",
    "    {\n",
    "        int age = 30; // Change the age value as needed\n",
    "\n",
    "        // Categorize the age without any console output\n",
    "        if (age < 18)\n",
    "        {\n",
    "            // Age is under 18\n",
    "        }\n",
    "        else if (age >= 18 && age < 65)\n",
    "        {\n",
    "            // Age is an adult\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            // Age is a senior citizen\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "c_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.CSHARP, chunk_size=128, chunk_overlap=0\n",
    ")\n",
    "c_docs = c_splitter.create_documents([C_CODE])\n",
    "c_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d650e476971e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b02758b0a7d16",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## MarkdownHeaderTextSplitter\n",
    "\n",
    "Many chat or Q+A applications involve chunking input documents prior to embedding and vector storage.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934e07e39939c4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For example these notes from Pinecone provide some useful tips:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c9ef499c1ae2830",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "When a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee6932580856e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As mentioned, chunking often aims to keep text with common context together. With this in mind, we might want to specifically honor the structure of the document itself. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use MarkdownHeaderTextSplitter. This will split a markdown file by a specified set of headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34c90c1c35f1540a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.432732600Z",
     "start_time": "2024-02-04T15:32:06.115585600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 'Header 1'), ('##', 'Header 2')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = '# Foo\\n\\n ## Bar\\n\\nHi this is Jim  \\nHi this is Joe\\n\\n ## Baz\\n\\n Hi this is Molly' \n",
    "\n",
    "[(\"#\", \"Header 1\"),(\"##\", \"Header 2\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f736f7c94792792",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And content is grouped or split by common headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52538e572cf8c40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.434732700Z",
     "start_time": "2024-02-04T15:32:06.121874600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Hi this is Molly',\n",
       " 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'content': 'Hi this is Jim  \\nHi this is Joe', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Bar'}}\n",
    "{'content': 'Hi this is Molly', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e4c0dd7d15300db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.448252300Z",
     "start_time": "2024-02-04T15:32:06.129523500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\n",
       " Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\n",
       " Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "680daebf57ab3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.450252500Z",
     "start_time": "2024-02-04T15:32:06.137028200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(md_header_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32660f4d924325",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "By default, ```MarkdownHeaderTextSplitter``` strips headers being split on from the output chunk’s content. This can be disabled by setting ```strip_headers = False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a8f28e3de69f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.452295900Z",
     "start_time": "2024-02-04T15:32:06.145865400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Foo  \\n## Bar  \\nHi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\n",
       " Document(page_content='### Boo  \\nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\n",
       " Document(page_content='## Baz  \\nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd7cf8f57ed60b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Within each markdown group we can then apply any text splitter we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "905784c99e6b39ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.454293700Z",
     "start_time": "2024-02-04T15:32:06.152097100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\n",
       " Document(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),\n",
       " Document(page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\n",
       " Document(page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),\n",
       " Document(page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd629b5892e9c017",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4ad45bec62fea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Recursively split by character\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is ```[\"\\n\\n\", \"\\n\", \" \", \"\"]```. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "1. How the text is split: by list of characters.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a440851e3c0d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.455796900Z",
     "start_time": "2024-02-04T15:32:06.159740100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'\n",
      "page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "306841fdfb80d10d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:06.457800800Z",
     "start_time": "2024-02-04T15:32:06.169076800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and',\n",
       " 'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(state_of_the_union)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621d586b1729e73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b3078fdb42e2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Semantic Chunking\n",
    "\n",
    "Splits the text based on semantic similarity.\n",
    "\n",
    "Taken from Greg Kamradt’s wonderful notebook: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "All credit to him.\n",
    "\n",
    "At a high level, this splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4333928de014ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:07.701940300Z",
     "start_time": "2024-02-04T15:32:06.174414600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc71337018c67d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:07.705942400Z",
     "start_time": "2024-02-04T15:32:07.702433800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81851a1ef714d43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:07.709448900Z",
     "start_time": "2024-02-04T15:32:07.704943200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d96958d37e618cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:10.670380900Z",
     "start_time": "2024-02-04T15:32:07.708448800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament Light will win over darkness. The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107713ab98480f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f057abc54d7a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Split by tokens\n",
    "\n",
    "Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c41e2c7270582f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### tiktoken\n",
    "\n",
    "[tiktoken](https://github.com/openai/tiktoken) is a fast BPE tokenizer created by OpenAI.\n",
    "\n",
    "We can use it to estimate tokens used. It will probably be more accurate for the OpenAI models.\n",
    "\n",
    "How the text is split: by character passed in.\n",
    "How the chunk size is measured: by tiktoken tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceb7e917c4b0ae4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.093657500Z",
     "start_time": "2024-02-04T15:32:10.670380900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea18c0435b1994e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.203668600Z",
     "start_time": "2024-02-04T15:32:12.092656800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7e9cde28bdd4485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.207176100Z",
     "start_time": "2024-02-04T15:32:12.203668600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632076db773a27b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that if we use ```CharacterTextSplitter.from_tiktoken_encoder```, text is only split by ```CharacterTextSplitter``` and tiktoken tokenizer is used to merge splits. It means that split can be larger than chunk size measured by tiktoken tokenizer. We can use ```RecursiveCharacterTextSplitter.from_tiktoken_encoder``` to make sure splits are not larger than chunk size of tokens allowed by the language model, where each split will be recursively split if it has a larger size.\n",
    "\n",
    "We can also load a tiktoken splitter directly, which ensure each split is smaller than chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e0373d9c6c7bf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.224232800Z",
     "start_time": "2024-02-04T15:32:12.206171200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff646966cfd4eb4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290237ecb5764a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n",
    "\n",
    "Another alternative to NLTK is to use [spaCy tokenizer](https://spacy.io/api/tokenizer).\n",
    "\n",
    "1. How the text is split: by spaCy tokenizer.\n",
    "2. How the chunk size is measured: by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea5ee023d503f567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.226370900Z",
     "start_time": "2024-02-04T15:32:12.217903600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  spacy\n",
    "# The following packages are needed to run SpacyTextSplitter\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6017dcaa80b4e7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.326283Z",
     "start_time": "2024-02-04T15:32:12.220233600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57258e6449d0448d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.978137300Z",
     "start_time": "2024-02-04T15:32:12.228377Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n",
      "\n",
      "Members of Congress and the Cabinet.\n",
      "\n",
      "Justices of the Supreme Court.\n",
      "\n",
      "My fellow Americans.  \n",
      "\n",
      "\n",
      "\n",
      "Last year COVID-19 kept us apart.\n",
      "\n",
      "This year we are finally together again. \n",
      "\n",
      "\n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents.\n",
      "\n",
      "But most importantly as Americans. \n",
      "\n",
      "\n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "\n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "\n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n",
      "\n",
      "But he badly miscalculated. \n",
      "\n",
      "\n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over.\n",
      "\n",
      "Instead he met a wall of strength he never imagined. \n",
      "\n",
      "\n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "\n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e89d9b3cc7a43b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb1f3145c26cfd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### SentenceTransformers\n",
    "\n",
    "The ```SentenceTransformersTokenTextSplitter``` is a specialized text splitter for use with the ```sentence-transformer models```. \n",
    "The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b478d94d90cafe39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:12.979136300Z",
     "start_time": "2024-02-04T15:32:12.976629900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cbab7a3cab714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:32:13.119646500Z",
     "start_time": "2024-02-04T15:32:12.978137300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\n",
    "text = \"Lorem \"\n",
    "\n",
    "count_start_and_stop_tokens = 2\n",
    "text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
    "print(text_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9934e01e0d08c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-04T15:32:13.119646500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1\n",
    "\n",
    "# `text_to_split` does not fit in a single chunk\n",
    "text_to_split = text * token_multiplier\n",
    "\n",
    "print(f\"tokens in text to split: {splitter.count_tokens(text=text_to_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b184b44cad27a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=text_to_split)\n",
    "\n",
    "print(text_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d74a9edc5c533",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c61b50d71c9852",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### NLTK\n",
    "\n",
    "The [Natural Language Toolkit](https://en.wikipedia.org/wiki/Natural_Language_Toolkit), or more commonly [NLTK](https://www.nltk.org/), is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.\n",
    "\n",
    "Rather than just splitting on “”, we can use NLTK to split based on NLTK tokenizers.\n",
    "\n",
    "1. How the text is split: by NLTK tokenizer.\n",
    "2. How the chunk size is measured: by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8b4e447a134d8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:42:49.249115800Z",
     "start_time": "2024-02-04T15:42:47.943788700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e2df565bb3fb3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:43:39.986740700Z",
     "start_time": "2024-02-04T15:43:39.926148400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26f99eb5cefe4052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:43:57.083181200Z",
     "start_time": "2024-02-04T15:43:57.010281500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n",
      "\n",
      "Members of Congress and the Cabinet.\n",
      "\n",
      "Justices of the Supreme Court.\n",
      "\n",
      "My fellow Americans.\n",
      "\n",
      "Last year COVID-19 kept us apart.\n",
      "\n",
      "This year we are finally together again.\n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents.\n",
      "\n",
      "But most importantly as Americans.\n",
      "\n",
      "With a duty to one another to the American people to the Constitution.\n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny.\n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n",
      "\n",
      "But he badly miscalculated.\n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over.\n",
      "\n",
      "Instead he met a wall of strength he never imagined.\n",
      "\n",
      "He met the Ukrainian people.\n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n",
      "\n",
      "Groups of citizens blocking tanks with their bodies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1837c38dbeae65",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96152fed44b83c9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Hugging Face tokenizer\n",
    "\n",
    "[Hugging Face](https://huggingface.co/docs/tokenizers/index) has many tokenizers.\n",
    "\n",
    "We use Hugging Face tokenizer, the [GPT2TokenizerFast](https://huggingface.co/Ransaka/gpt2-tokenizer-fast) to count the text length in tokens.\n",
    "\n",
    "1. How the text is split: by character passed in.\n",
    "2. How the chunk size is measured: by number of tokens calculated by the Hugging Face tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ff577913a4c13de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:47:33.133105800Z",
     "start_time": "2024-02-04T15:47:23.417917700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in p:\\projects\\llm-frameworks-tutorials\\langchain tutorials\\tutorials from langchain\\.venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.4 MB 1.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/8.4 MB 5.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.1/8.4 MB 10.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.9/8.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.9/8.4 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.9/8.4 MB 15.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.1/8.4 MB 17.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.8/8.4 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.3/8.4 MB 21.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 20.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp312-none-win_amd64.whl (270 kB)\n",
      "   ---------------------------------------- 0.0/270.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 270.7/270.7 kB 17.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, transformers\n",
      "Successfully installed safetensors-0.4.2 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30314f599f10adf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:47:51.379594100Z",
     "start_time": "2024-02-04T15:47:48.856369900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b544f61d04194a9d9a807f532fbd9917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b58d0e7d914eae99bf8886fcc51060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a630f8571d44981b6c6ab2248ece32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db05bfafebf42338b5342850c82fa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a949d0fbc0543e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:48:54.360694900Z",
     "start_time": "2024-02-04T15:48:54.275463Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89c2c8f233977f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
