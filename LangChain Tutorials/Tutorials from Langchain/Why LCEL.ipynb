{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bad9c7fa3d1a1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why Use LCEL\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/10/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4026e7b25f8708e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "**In this project:**  I compare using LCEL and using LangChain without LCEL.   \n",
    "LCEL makes it easier to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAi key.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f1978221b051a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Why LCEL**\n",
    "\n",
    "**LangChain Expression Language**, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to **support putting prototypes in production, with no code changes,** from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n",
    "\n",
    "**Streaming support** When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n",
    "\n",
    "**Async support** Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a [LangServe](https://python.langchain.com/docs/langsmith) server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\n",
    "\n",
    "**Optimized parallel execution** Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\n",
    "\n",
    "**Retries and fallbacks** Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\n",
    "\n",
    "**Access intermediate** results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every [LangServe](https://python.langchain.com/docs/langsmith) server.\n",
    "\n",
    "**Input and output schemas** Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\n",
    "\n",
    "**Seamless LangSmith tracing integration** As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n",
    "\n",
    "**Seamless LangServe deployment integration** Any chain created with LCEL can be easily deployed using [LangServe](https://python.langchain.com/docs/langsmith).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158c35cb57541ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project map:\n",
    "- [API Key](#api-key)\n",
    "- [Basic Example Using LCEL](#basic-example-using-lcel)\n",
    "- [Invoke](#invoke)\n",
    "    - [With LCEL](#with-lcel-invoke)\n",
    "    - [Without LCEL](#without-lcel-invoke)\n",
    "- [Stream](#stream)\n",
    "    - [With LCEL](#with-lcel-steam)\n",
    "    - [Without LCEL](#without-lcel-steam)\n",
    "- [Batch](#batch)\n",
    "    - [With LCEL](#with-lcel-batch)\n",
    "    - [Without LCEL](#without-lcel-batch)\n",
    "- [Async](#batch)\n",
    "    - [With LCEL](#with-lcel-async)\n",
    "    - [Without LCEL](#without-lcel-async)\n",
    "- [LLM instead of chat model](#llm-instead-of-chat-model)\n",
    "    - [With LCEL](#with-lcel-llm-instead-of-chat-model)\n",
    "    - [Without LCEL](#without-lcel-llm-instead-of-chat-model)\n",
    "- [Different model provider](#different-model-provider)\n",
    "    - [With LCEL](#with-lcel-different-model-provider)\n",
    "    - [Without LCEL](#without-lcel-different-model-provider)\n",
    "- [Runtime configurability](#runtime-configurability)\n",
    "    - [With LCEL](#with-lcel-runtime-configurability)\n",
    "    - [Without LCEL](#without-lcel-runtime-configurability)\n",
    "- [Logging](#logging)\n",
    "    - [With LCEL](#with-lcel-logging)\n",
    "    - [Without LCEL](#without-lcel-logging)\n",
    "- [Fallbacks](#fallbacks)\n",
    "    - [With LCEL](#with-lcel-fallbacks)\n",
    "    - [Without LCEL](#without-lcel-fallbacks)\n",
    "- [Full code comparison](#full-code-comparison)\n",
    "    - [With LCEL](#with-lcel-full-code-comparison)\n",
    "    - [Without LCEL](#without-lcel-full-code-comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41790e99ba398a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2ff73d4ffa2aea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:05:46.937146700Z",
     "start_time": "2024-01-19T15:05:46.920674900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c1c1119248ab2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Basic Example Using LCEL\n",
    "It is recommended to read the LCEL Get started [Intro LangChain LCEL.ipynb](https://github.com/Omegapy/LLM-Frameworks-Tutorials/blob/main/LangChain%20Tutorials/Tutorials%20from%20Langchain/Intro%20LangChain%20LCEL.ipynb) section first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b496753d89c93f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:05:34.798264700Z",
     "start_time": "2024-01-19T15:05:34.478741Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195bd14",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb2e977a10b858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933db44310e2282",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### With LCEL (Invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d5f15dcebcb6b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:44:57.115225700Z",
     "start_time": "2024-01-19T14:44:54.518136900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy?\\n\\nBecause it had too many sprinkles of anxiety!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ad72126983b57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Without LCEL (Invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "529c61ef8166cc0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:46:54.278977200Z",
     "start_time": "2024-01-19T14:46:51.247946300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy?\\n\\nBecause it had too many cold sprinkles!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdeb2c6",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90aa255336d6c7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stream\n",
    "Stream breacks up the AImessage in sentenses (chucks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4c170",
   "metadata": {},
   "source": [
    "##### With LCEL (Steam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ced12b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream go to therapy?\n",
      "Because it had too many sprinkles of anxiety!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b6e54",
   "metadata": {},
   "source": [
    "##### Without LCEL (Steam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f89cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a joke for you: Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "\n",
    "\n",
    "for chunk in stream_chain(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812105a",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c146d7e",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Running a batch of inputs in parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3796be",
   "metadata": {},
   "source": [
    "##### With LCEL (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "339d9c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the ice cream go to therapy?\\n\\nBecause it had too many toppings and couldn't handle the sprinkles!\",\n",
       " 'Why did the tomato turn red?\\n\\nBecause it saw the spaghetti sauce!',\n",
       " 'Why did the dumpling go to the party?\\n\\nBecause it wanted to get stuffed!']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]) # Note the three diferent prompt inputes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3becc",
   "metadata": {},
   "source": [
    "Without LCEL (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60ce6b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ice cream go to therapy?\\nBecause it always felt a little meltdramatic!',\n",
       " 'Why did the tomato turn red?\\n\\nBecause it saw the spaghetti sauce!',\n",
       " 'Why did the dumpling go to therapy?\\n\\nBecause it had too many emotional fillings!']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def batch_chain(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(invoke_chain, topics))\n",
    "\n",
    "batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb705a14",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796e7f6",
   "metadata": {},
   "source": [
    "### Async\n",
    "\n",
    "Runs chains asynchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a53bf",
   "metadata": {},
   "source": [
    "#### With LCEL (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9e7e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to the party? \\nBecause it was feeling a little \"meltdown\"!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9f94f",
   "metadata": {},
   "source": [
    "#### Without LCEL (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0e82178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy?\\n\\nBecause it had too many emotional sundaes!'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_client = openai.AsyncOpenAI()\n",
    "\n",
    "async def acall_chat_model(messages: List[dict]) -> str:\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def ainvoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return await acall_chat_model(messages)\n",
    "\n",
    "await ainvoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85178d76",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664dc46",
   "metadata": {},
   "source": [
    "### LLM instead of chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfcb88",
   "metadata": {},
   "source": [
    "#### With LCEL (LLM instead of chat model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5c7e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the ice cream go to therapy?\\n\\nBecause it was feeling a little melon-choly.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "llm_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51f831",
   "metadata": {},
   "source": [
    "#### Without LCEL (LLM instead of chat model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51bd6e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle ride to get ice cream?\\n\\nBecause it was two\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_llm(prompt_value: str) -> str:\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "def invoke_llm_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    return call_llm(prompt_value)\n",
    "\n",
    "invoke_llm_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1d37e",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dbd69",
   "metadata": {},
   "source": [
    "### Different model provider\n",
    "\n",
    "Anthropic instead of OpenAI\n",
    "Note: this is just an example I do not have an Anthropic API Key -> code has no ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c721cf",
   "metadata": {},
   "source": [
    "#### With LCEL (Different model provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65643790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "anthropic_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | anthropic\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa3356",
   "metadata": {},
   "source": [
    "#### Without LCEL (Different model provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "def call_anthropic(prompt_value: str) -> str:\n",
    "    response = anthropic_client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    return response.completion    \n",
    "\n",
    "def invoke_anthropic_chain(topic: str) -> str:\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    return call_anthropic(prompt_value)\n",
    "\n",
    "invoke_anthropic_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a67bed",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73be53",
   "metadata": {},
   "source": [
    "### Runtime configurability\n",
    "\n",
    "If we wanted to make the choice of chat model or LLM configurable at runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c17eb",
   "metadata": {},
   "source": [
    "#### With LCEL (Runtime configurability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=llm,\n",
    "    anthropic=anthropic,\n",
    ")\n",
    "\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "configurable_chain.invoke(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"openai\"}\n",
    ")\n",
    "stream = configurable_chain.stream(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"anthropic\"}\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
    "\n",
    "await configurable_chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a21ce2",
   "metadata": {},
   "source": [
    "#### Without LCEL (Runtime configurability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> str:\n",
    "    if model == \"chat_openai\":\n",
    "        return invoke_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        return invoke_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        return invoke_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def stream_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    if model == \"chat_openai\":\n",
    "        return stream_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        # Note we haven't implemented this yet.\n",
    "        return stream_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        # Note we haven't implemented this yet\n",
    "        return stream_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def batch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    # You get the idea\n",
    "    ...\n",
    "\n",
    "async def abatch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "invoke_configurable_chain(\"ice cream\", model=\"openai\")\n",
    "stream = stream_configurable_chain(\n",
    "    \"ice_cream\", \n",
    "    model=\"anthropic\"\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
    "# await ainvoke_configurable_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8cfde",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579954ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0f1e06",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "Logging into LangChain using a LangChain API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56117f7",
   "metadata": {},
   "source": [
    "#### With LCEL (Logging)\n",
    "\n",
    "Note: I do note have a LangChain API Key (https://smith.langchain.com/o/bd3fb686-7fb6-557c-95f9-8f1d8dcb7aee/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9110ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e1e15",
   "metadata": {},
   "source": [
    "#### Without LCEL (Logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_anthropic_chain_with_logging(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_anthropic(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "invoke_anthropic_chain_with_logging(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9c544",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49067424",
   "metadata": {},
   "source": [
    "### Fallbacks\n",
    "\n",
    "If we wanted to add fallback logic, in case one model API is down\n",
    "(https://python.langchain.com/docs/guides/fallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b688ede",
   "metadata": {},
   "source": [
    "#### With LCEL (Fallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7cdbb7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring a meltdown!\",\n",
       " 'Why don’t we tell secrets on a farm?\\n\\nBecause the potatoes have eyes, the corn has ears, and the spaghetti is pasta-tively terrible at keeping secrets!',\n",
       " \"Why don't dumplings ever win at hide and seek?\\n\\nBecause they always meet at the steamer!\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# good chain\n",
    "chat_openai = ChatOpenAI(model=\"gpt-4\")\n",
    "good_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | chat_openai\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "# Note if bad_chain fails good chain will run\n",
    "fallback_chain = bad_chain.with_fallbacks([good_chain])\n",
    "\n",
    "# fallback_chain.invoke(\"ice cream\")\n",
    "# or\n",
    "# await fallback_chain.ainvoke(\"ice cream\")\n",
    "# or\n",
    "fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1a96a",
   "metadata": {},
   "source": [
    "#### Without LCEL (Fallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "451a8045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ice cream go to the party? \\nBecause it was looking to get \"scooped\" up and have a \"cone\" of a good time!',\n",
       " 'Why did the spaghetti bring a ladder? \\n\\nBecause it wanted to \"pasta\" sauce!',\n",
       " 'Why did the dumpling refuse to have dessert?\\n\\nBecause it was already stuffed!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def invoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return invoke_chain(topic)\n",
    "    except Exception:\n",
    "        return invoke_anthropic_chain(topic)\n",
    "\n",
    "async def ainvoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return await ainvoke_chain(topic)\n",
    "    except Exception:\n",
    "        # Note: we haven't actually implemented this.\n",
    "        return ainvoke_anthropic_chain(topic)\n",
    "\n",
    "async def batch_chain_with_fallback(topics: List[str]) -> str:\n",
    "    try:\n",
    "        return batch_chain(topics)\n",
    "    except Exception:\n",
    "        # Note: we haven't actually implemented this.\n",
    "        return batch_anthropic_chain(topics)\n",
    "\n",
    "#invoke_chain_with_fallback(\"ice cream\")\n",
    "# or\n",
    "# await ainvoke_chain_with_fallback(\"ice cream\")\n",
    "# or\n",
    "await batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c940b3",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1ec63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6badbc90",
   "metadata": {},
   "source": [
    "### Full code comparison\n",
    "\n",
    "You need an Anthropic and LangChain API key to run the full code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973e136",
   "metadata": {},
   "source": [
    "#### With LCEL (Full code comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([anthropic])\n",
    "    .configurable_alternatives(\n",
    "        ConfigurableField(id=\"model\"),\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52ae41",
   "metadata": {},
   "source": [
    "#### Without LCEL (Full code comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b465df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import anthropic\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\n",
    "client = openai.OpenAI()\n",
    "async_client = openai.AsyncOpenAI()\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    output = call_chat_model(messages)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "    for chunk in stream:\n",
    "        print(f\"Token: {chunk}\", end=\"\")\n",
    "        yield chunk\n",
    "\n",
    "def batch_chain(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(invoke_chain, topics))\n",
    "\n",
    "def call_llm(prompt_value: str) -> str:\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "def invoke_llm_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = promtp_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_llm(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "def call_anthropic(prompt_value: str) -> str:\n",
    "    response = anthropic_client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    return response.completion   \n",
    "\n",
    "def invoke_anthropic_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_anthropic(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "async def ainvoke_anthropic_chain(topic: str) -> str:\n",
    "    ...\n",
    "\n",
    "def stream_anthropic_chain(topic: str) -> Iterator[str]:\n",
    "    ...\n",
    "\n",
    "def batch_anthropic_chain(topics: List[str]) -> List[str]:\n",
    "    ...\n",
    "\n",
    "def invoke_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> str:\n",
    "    if model == \"chat_openai\":\n",
    "        return invoke_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        return invoke_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        return invoke_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def stream_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    if model == \"chat_openai\":\n",
    "        return stream_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        # Note we haven't implemented this yet.\n",
    "        return stream_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        # Note we haven't implemented this yet\n",
    "        return stream_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def batch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "async def abatch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "def invoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return invoke_chain(topic)\n",
    "    except Exception:\n",
    "        return invoke_anthropic_chain(topic)\n",
    "\n",
    "async def ainvoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return await ainvoke_chain(topic)\n",
    "    except Exception:\n",
    "        return ainvoke_anthropic_chain(topic)\n",
    "\n",
    "async def batch_chain_with_fallback(topics: List[str]) -> str:\n",
    "    try:\n",
    "        return batch_chain(topics)\n",
    "    except Exception:\n",
    "        return batch_anthropic_chain(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ad6b4",
   "metadata": {},
   "source": [
    "[Project map](#project-map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
