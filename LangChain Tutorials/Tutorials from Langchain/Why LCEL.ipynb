{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bad9c7fa3d1a1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why Use LCEL\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/10/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4026e7b25f8708e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "**In this project:**  I compare using LCEL and using LangChain without LCEL.   \n",
    "LCEL makes it easier to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAi key.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f1978221b051a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Why LCEL**\n",
    "\n",
    "**LangChain Expression Language**, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to **support putting prototypes in production, with no code changes,** from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n",
    "\n",
    "**Streaming support** When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n",
    "\n",
    "**Async support** Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a [LangServe](https://python.langchain.com/docs/langsmith) server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\n",
    "\n",
    "**Optimized parallel execution** Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\n",
    "\n",
    "**Retries and fallbacks** Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\n",
    "\n",
    "**Access intermediate** results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every [LangServe](https://python.langchain.com/docs/langsmith) server.\n",
    "\n",
    "**Input and output schemas** Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\n",
    "\n",
    "**Seamless LangSmith tracing integration** As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n",
    "\n",
    "**Seamless LangServe deployment integration** Any chain created with LCEL can be easily deployed using [LangServe](https://python.langchain.com/docs/langsmith).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158c35cb57541ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Project map:\n",
    "- [API Key](#api-key)\n",
    "- [Basic Example Using LCEL](#basic-example-using-lcel)\n",
    "- [Invoke](#invoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41790e99ba398a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ff73d4ffa2aea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:05:46.937146700Z",
     "start_time": "2024-01-19T15:05:46.920674900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c1c1119248ab2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Basic Example Using LCEL\n",
    "It is recommended to read the LCEL Get started [Intro LangChain LCEL.ipynb](https://github.com/Omegapy/LLM-Frameworks-Tutorials/blob/main/LangChain%20Tutorials/Tutorials%20from%20Langchain/Intro%20LangChain%20LCEL.ipynb) section first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b496753d89c93f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:05:34.798264700Z",
     "start_time": "2024-01-19T15:05:34.478741Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb2e977a10b858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933db44310e2282",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### With LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5f15dcebcb6b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:44:57.115225700Z",
     "start_time": "2024-01-19T14:44:54.518136900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything, but ice cream is the exception!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ad72126983b57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529c61ef8166cc0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:46:54.278977200Z",
     "start_time": "2024-01-19T14:46:51.247946300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? \\n\\nBecause it was feeling a little melty!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90aa255336d6c7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
