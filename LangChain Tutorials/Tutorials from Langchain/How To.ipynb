{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How To\n",
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "---\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/20/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/) \n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f143f1fb2ae8f71f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models. \n",
    "\n",
    "**In this project:** \n",
    "I explore how to apply LCEL to different code applications such as manipulate data and add massage history.\n",
    "\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAi key.\n",
    "</b>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed51e2cbcaf941b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Project Map:\n",
    "- [API Key](#api-key)\n",
    "- [Manipulating Inputs Output (RunnableParallel)](#manipulating-inputs-output-runnableparallel)\n",
    "    - [Base Example](#base-example)\n",
    "    - [Using itemgetter as shorthand](#using-itemgetter-as-shorthand)\n",
    "    - [Parallelism](#paralleliism)\n",
    "        - [Base Example](#base-example-parallelism)\n",
    "        - [Parallelism running independent processes](#parallelism-running-independent-processes)\n",
    "- [Passing data through (RunnablePassthrough)](#passing-data-through-runnablepassthrough)\n",
    "    - [Base Example](#base-example-passing-data-through)\n",
    "    - [Retrieval Example](#retrieval-example)\n",
    "- [Run Custom Functions](#run-custom-functions)\n",
    "    - [Base Example](#base-example-custom-functions)\n",
    "    - [Accepting a Runnable Config - (Token Used)](#accepting-a-runnable-config---token-used)\n",
    "- [Dynamically route logic based on input (RunnableBranch)](#dynamically-route-logic-based-on-input-runnablebranch)\n",
    "    - [Using a RunnableBranch](#using-a-runnablebranch)\n",
    "    - [Using a custom function to route](#using-a-custom-function-to-route)\n",
    "- [Bind runtime args](#bind-runtime-args)\n",
    "    - [Base Example](#base-example-bind-runtime-args)\n",
    "    - [Attaching OpenAI functions](#attaching-openai-functions)\n",
    "    - [Attaching OpenAI tools](#attaching-openai-tools)\n",
    "    - [Configuration Fields](#configuration-fields)\n",
    "        - [With LLMs (llm temp)](#with-llms-llm-temp)\n",
    "        - [With HubRunnables (prompt switching)](#with-hubrunnables-prompt-switching)\n",
    "    - [Configurable Alternatives](#configurable-alternatives)\n",
    "        - [With LLMs (model Alt)](#with-llms-model-alt)\n",
    "        - [With Prompts](#with-prompts-conf-alt)\n",
    "        - [With Prompts and LLMs](#with-prompts-and-llms)\n",
    "    - [Saving configurations](#saving-configurations)\n",
    "        \n",
    "\n",
    "\n",
    "- []()\n",
    "    \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0fbbf3066fa4eee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API Key"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7189a3768154ee1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:27:29.444969400Z",
     "start_time": "2024-01-21T01:27:29.401821Z"
    }
   },
   "id": "300cf0b10a020f9b",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a032b6821b38fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Manipulating Inputs Output (RunnableParallel)\n",
    "\n",
    "``RunnableParallel`` can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.\n",
    "\n",
    "Here the input to prompt is expected to be a map with keys ```context``` and ```question```.  \n",
    "The **user input is just the ```question```**.  \n",
    "So we need to **get the ```context``` using our ```retriever```** and **```passthrough``` the user input under the ```question``` key**.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959a260f754100f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54147d707f4392e6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Harrison worked at Kensho.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS # pip install faiss-cpu\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Vectorstore in RAM\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], # Stores in RAM\n",
    "    embedding=OpenAIEmbeddings() \n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:06:04.622914700Z",
     "start_time": "2024-01-20T19:06:02.872110100Z"
    }
   },
   "id": "81fdd3fa79463021",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'context': VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000013E5DB99FD0>),\n 'question': RunnablePassthrough()}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"context\": retriever, \"question\": RunnablePassthrough()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:02:04.260928600Z",
     "start_time": "2024-01-20T19:02:04.226954700Z"
    }
   },
   "id": "ee81eed2af6d8824",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "RunnableParallel (aka. RunnableMap)\n",
    "[class langchain_core.runnables.base.RunnableParallel](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.RunnableParallel.html?highlight=runnableparallel#langchain_core.runnables.base.RunnableParallel)\n",
    "Two different syntaxes\n",
    "A runnable that runs a mapping of runnables in parallel, and returns a mapping of their outputs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10047dc8d119074e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000013F380C7910>),\n  question: RunnablePassthrough()\n}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:06:09.820186300Z",
     "start_time": "2024-01-20T19:06:09.810354600Z"
    }
   },
   "id": "420c404cf8f8387f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000013F380C7910>),\n  question: RunnablePassthrough()\n}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:06:17.625070800Z",
     "start_time": "2024-01-20T19:06:17.616758100Z"
    }
   },
   "id": "3b00732be52036a5",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d11a101df161bf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using itemgetter as shorthand\n",
    "\n",
    "Using ```itemgetter()``` instated of ```RunnablePassthrough()```\n",
    "\n",
    "Note that you can use Python’s ```itemgetter``` as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about [itemgetter](https://docs.python.org/3/library/operator.html#operator.itemgetter) in the Python Documentation.\n",
    "\n",
    "In the example below, we use itemgetter to extract specific keys from the map:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b8b969a885cfd93"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Harrison ha lavorato a Kensho.'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:16:49.853517200Z",
     "start_time": "2024-01-20T19:16:48.245944100Z"
    }
   },
   "id": "4643a14fe99c9666",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30bbc67a78380892"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallelism\n",
    "\n",
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3611da008825cb4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Base Example (Parallelism)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73395bdfbe613a37"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n 'poem': AIMessage(content=\"In forest's embrace, bear prowls with might,\\nA wild guardian, ruling day and night.\")}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:42:20.156931600Z",
     "start_time": "2024-01-20T20:42:18.335579800Z"
    }
   },
   "id": "cd07c1364a757534",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54279218d5463bd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Parallelism running independent processes \n",
    "\n",
    "**RunnableParallel are also useful for running independent processes in parallel**, since each Runnable in the map is executed in parallel. For example, we can see our earlier ``joke_chain``, ```poem_chain``` and ```map_chain``` all have about the same runtime, even though map_chain executes both of the other two."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e5836b1f1c3a6f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19 s ± 228 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:54:08.587950500Z",
     "start_time": "2024-01-20T20:53:58.494039100Z"
    }
   },
   "id": "50fec6c1b663247d",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 s ± 300 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:54:20.893769300Z",
     "start_time": "2024-01-20T20:54:08.579889900Z"
    }
   },
   "id": "42fd2e7ac63d819",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.45 s ± 82.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Execute both the joke_chain and poem_chain\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:53:48.458590900Z",
     "start_time": "2024-01-20T20:53:36.946410Z"
    }
   },
   "id": "da87bc8d00fde20d",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dae11d86c34669c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Passing data through (RunnablePassthrough)\n",
    "\n",
    "```RunnablePassthrough``` allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with ```RunnableParallel``` to assign data to a new key in the map.\n",
    "\n",
    "```RunnablePassthrough()``` called on it’s own, will simply take the input and pass it through.\n",
    "\n",
    "RunnablePassthrough called with assign (```RunnablePassthrough.assign(...)```) will take the input, and will add the extra arguments passed to the assign function.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce3d0bf96cb9bbb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Base Example (Passing data through)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc9ce77f2a5260d8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:07:31.789892900Z",
     "start_time": "2024-01-20T21:07:31.766857500Z"
    }
   },
   "id": "d4f4d88164931f20",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "As seen above, passed key was called with ```RunnablePassthrough()``` and so it simply passed on ```{'num': 1}```.\n",
    "\n",
    "In the second line, we used ```RunnablePastshrough.assign```.assign with a lambda that multiplies the numerical value by ``3``. In this cased, extra was set with ```{'num': 1, 'mult': 3}``` which is the original value with the ```mult``` key added.\n",
    "\n",
    "Finally, we also set a third key in the map with ```modified``` which uses a lambda to set a single value adding ```1``` to the ```num```, which resulted in ```modified``` key with the value of ```2```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ba0335eec21ae26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b111991c800551"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieval Example\n",
    "\n",
    "In the example below, we see a use case where we use RunnablePassthrough along with RunnableMap."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "391177f8a9f0964d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Harrison worked at Kensho.'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:53:05.465151700Z",
     "start_time": "2024-01-20T21:53:02.734032100Z"
    }
   },
   "id": "99b923b2b17971b3",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the input to prompt is expected to be a map with keys ```context``` and ```question```. \n",
    "The user input is just the ```question```. \n",
    "So we need to get the context using our ```retriever``` and ```passthrough``` the user input under the ```question```key. \n",
    "In this case, the ```RunnablePassthrough``` allows us to pass on the user’s question to the ```prompt``` and ```model```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0123a5f4b0216a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5aefc621651de33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Run Custom Functions\n",
    "\n",
    "You can use arbitrary functions in the pipeline.\n",
    "\n",
    "Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0faa67da2e25ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Example (Custom Functions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "399ee37f4753228c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:02:49.168314200Z",
     "start_time": "2024-01-20T22:02:48.799356Z"
    }
   },
   "id": "d3044d2cc346107f",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='3 + 9 equals 12.')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:03:01.646317Z",
     "start_time": "2024-01-20T22:03:00.796446100Z"
    }
   },
   "id": "509db96b4d7d1852",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f25cc46becd7ab39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accepting a Runnable Config - (Token Used)\n",
    "Runnable lambdas can optionally accept a [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig), which they can use to pass callbacks, tags, and other configuration information to nested runs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b752194b8221da1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:05:45.460591600Z",
     "start_time": "2024-01-20T22:05:45.418061500Z"
    }
   },
   "id": "eba413c69da5d3df",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n",
      "Tokens Used: 65\n",
      "\tPrompt Tokens: 56\n",
      "\tCompletion Tokens: 9\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00010200000000000001\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI()\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\"\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "    print(output)\n",
    "    print(cb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:05:58.702780600Z",
     "start_time": "2024-01-20T22:05:57.380568400Z"
    }
   },
   "id": "2e7abd0c03687fff",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "547aef875ef6d1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Dynamically route logic based on input (RunnableBranch)\n",
    "\n",
    "how to do routing in the LangChain Expression Language.\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.\n",
    "\n",
    "There are two ways to perform routing:\n",
    "1. Using a ```RunnableBranch```.\n",
    "2. Writing custom factory function that takes the input of a previous step and returns a runnable. Importantly, this should return a runnable and NOT actually execute.\n",
    "\n",
    "We’ll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f10bb365befdca0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using a RunnableBranch\n",
    "A ```RunnableBranch``` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it’s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "\n",
    "If no provided conditions match, it runs the default runnable.\n",
    "\n",
    "Here’s an example of what it looks like in action:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce899cbe61e7bb2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**If no provided conditions match, it runs the default runnable.**\n",
    "\n",
    "Here’s an example of what it looks like in action:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb73d4c355d6d103"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:13:25.414921600Z",
     "start_time": "2024-01-20T23:13:25.063009800Z"
    }
   },
   "id": "b03438e115d50fb1",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "Given the user question below, classify it as either being about `LangChain`, `Anthropic`, `OpenAI`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:17:33.218007Z",
     "start_time": "2024-01-20T23:17:32.888097Z"
    }
   },
   "id": "9faf83c7bd6927b8",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'OpenAI'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"how do I call OpenAI?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:17:36.953470500Z",
     "start_time": "2024-01-20T23:17:35.274321500Z"
    }
   },
   "id": "87e2ed889eeffec8",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let’s create three sub chains:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9debbf7e4a600caa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "langchain_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "anthropic_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "openai_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in openai. \\\n",
    "Always answer questions starting with \"As Ilya Sutskever told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:27:11.732734200Z",
     "start_time": "2024-01-20T23:27:10.355621200Z"
    }
   },
   "id": "c9527cc4279471d9",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "    (lambda x: \"openai\" in x[\"topic\"].lower(), openai_chain),\n",
    "    general_chain,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:27:12.752866700Z",
     "start_time": "2024-01-20T23:27:12.710822300Z"
    }
   },
   "id": "88e5e695ad1f0c7f",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='As Dario Amodei told me, using Anthropic involves understanding and applying the principles of anthropic reasoning. This includes considering the observer selection effects and taking into account the fact that our observations are influenced by the conditions necessary for our existence. By incorporating anthropic reasoning into your analysis, you can better understand and make predictions about phenomena that are influenced by our presence as observers.')"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
    "\n",
    "full_chain.invoke({\"question\": \"how do I use Anthropic?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:28:03.075629500Z",
     "start_time": "2024-01-20T23:27:58.539566500Z"
    }
   },
   "id": "7bfe29453e2536fd",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"As Ilya Sutskever told me, using OpenAI involves several steps. First, you need to sign up for an API key on the OpenAI website. Once you have the API key, you can use it to make requests to the OpenAI API, which allows you to access various models and functionalities. You can make requests using programming languages like Python by sending HTTP POST requests to the appropriate API endpoint with your API key and the desired input. It's important to familiarize yourself with the API documentation to understand the available models, their capabilities, and any specific usage instructions. Additionally, OpenAI provides code examples and tutorials to help you get started with specific use cases.\")"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use OpenAI?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:31:55.021252800Z",
     "start_time": "2024-01-20T23:31:47.565101700Z"
    }
   },
   "id": "2260905044ed750d",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='The sum of 2 plus 2 is 4.')"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:32:00.570818300Z",
     "start_time": "2024-01-20T23:31:59.117637800Z"
    }
   },
   "id": "74cc04756bb775aa",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc05138f8157f7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using a custom function to route\n",
    "\n",
    "You can also use a custom function to route between different outputs. Here’s an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0f37c9056b26b4a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropic_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    elif \"openai\" in info[\"topic\"].lower():\n",
    "        return openai_chain\n",
    "    else:\n",
    "        return general_chain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:37:07.680818Z",
     "start_time": "2024-01-20T23:37:07.653808300Z"
    }
   },
   "id": "61d9e11e3d6042fe",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:37:25.296737100Z",
     "start_time": "2024-01-20T23:37:25.258947Z"
    }
   },
   "id": "8c79a182099954b8",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='As Dario Amodei told me, to use Anthropic, you can start by understanding the underlying principles and concepts of the field. This includes familiarizing yourself with the concept of the anthropic principle, which explores the relationship between the existence of intelligent observers and the fundamental properties of the universe. Additionally, you can study relevant literature and research papers to gain a deeper understanding of the field. As you delve further into the subject, you can also engage in discussions and collaborate with other researchers and experts in the field to enhance your knowledge and contribute to the advancement of Anthropics.')"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use Anthropic?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:37:43.916830800Z",
     "start_time": "2024-01-20T23:37:37.814720Z"
    }
   },
   "id": "942fa9b1fe97082b",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3b866ac26b19c5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Bind runtime args\n",
    "Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3166f98a334afbac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Example (Bind runtime args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4588d3d4b262f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:43:20.370479300Z",
     "start_time": "2024-01-20T23:43:20.333446700Z"
    }
   },
   "id": "a8488ed37813fb6d",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: x^3 + 7 = 12\n",
      "\n",
      "SOLUTION:\n",
      "Subtracting 7 from both sides of the equation, we get:\n",
      "x^3 = 12 - 7\n",
      "x^3 = 5\n",
      "\n",
      "Taking the cube root of both sides, we get:\n",
      "x = ∛5\n",
      "\n",
      "Therefore, the solution to the equation x^3 + 7 = 12 is x = ∛5.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0)\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:43:34.997469600Z",
     "start_time": "2024-01-20T23:43:29.849018500Z"
    }
   },
   "id": "af2ae03df82cf5c1",
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4ad02544582906a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attaching OpenAI functions\n",
    "One particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f83fbc7bb630e93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "function = {\n",
    "    \"name\": \"solver\",\n",
    "    \"description\": \"Formulates and solves an equation\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"equation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The algebraic expression of the equation\",\n",
    "            },\n",
    "            \"solution\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The solution to the equation\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],\n",
    "    },\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:12:11.382053600Z",
     "start_time": "2024-01-21T00:12:11.351179900Z"
    }
   },
   "id": "e159bf25cfec8a54",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"equation\": \"x^3 + 7 = 12\",\\n\"solution\": \"x = ∛5\"\\n}', 'name': 'solver'}})"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need gpt-4 to solve this one correctly\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it.\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind(\n",
    "    function_call={\"name\": \"solver\"}, functions=[function]\n",
    ")\n",
    "runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model\n",
    "runnable.invoke(\"x raised to the third plus seven equals 12\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:12:54.835535700Z",
     "start_time": "2024-01-21T00:12:52.755185900Z"
    }
   },
   "id": "7eabf4a0eafd3bd4",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b21330588cde285"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attaching OpenAI tools"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caccbdf8294e572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:13:53.172713600Z",
     "start_time": "2024-01-21T00:13:53.143190800Z"
    }
   },
   "id": "12b7026778f4fed1",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mdEMUhXmE7ml47daM54zOCkF', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_gPG3aWAqsuDndtGHenbqk7vy', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_xF4pRfZHs9P5H5SEPId4ViyT', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]})"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)\n",
    "model.invoke(\"What's the weather in SF, NYC and LA?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:14:06.013112100Z",
     "start_time": "2024-01-21T00:14:03.128579Z"
    }
   },
   "id": "92bd5a069027a397",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb020bf7113645d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Configure chain internals at runtime\n",
    "\n",
    "Oftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things. In order to make this experience as easy as possible, we have defined two methods.\n",
    "\n",
    "First, a ```configurable_fields``` method. This lets you configure particular fields of a runnable.\n",
    "\n",
    "Second, a ```configurable_alternatives``` method. With this method, you can list out alternatives for any particular runnable that can be set during runtime.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0051eb3fdff1413"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration Fields"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ef891ff7391a6ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### With LLMs (llm temp)\n",
    "With LLMs we can configure things like temperature"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36bcc30dc2b30899"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:19:20.221988Z",
     "start_time": "2024-01-21T00:19:19.458779900Z"
    }
   },
   "id": "98cebf90191442db",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='7')"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"pick a random number\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:19:41.687859900Z",
     "start_time": "2024-01-21T00:19:41.124233700Z"
    }
   },
   "id": "671341d95865d9ce",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='9')"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:19:57.765363800Z",
     "start_time": "2024-01-21T00:19:57.221664100Z"
    }
   },
   "id": "758216273d6ccecb",
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also do this when its used as part of a chain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b99e59d8c44e9509"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\n",
    "chain = prompt | model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:20:38.111540500Z",
     "start_time": "2024-01-21T00:20:38.078873300Z"
    }
   },
   "id": "18ad1015cc8f6021",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='57')"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"x\": 0})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:20:58.673272500Z",
     "start_time": "2024-01-21T00:20:57.087881100Z"
    }
   },
   "id": "c04956051a92b5df",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='83')"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:21:10.610459700Z",
     "start_time": "2024-01-21T00:21:10.113996200Z"
    }
   },
   "id": "61e80b9e2c554225",
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f62e0730e8d5ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### With HubRunnables (prompt switching)\n",
    "This is useful to allow for switching of prompts\n",
    "\n",
    "This are runnable from the [LangChain Hub](https://smith.langchain.com/hub?organizationId=bd3fb686-7fb6-557c-95f9-8f1d8dcb7aee) .\n",
    "for example:\n",
    "- [rlm/rag-prompt](https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=bd3fb686-7fb6-557c-95f9-8f1d8dcb7aee) is a 'RAG' ```ChatPromptTemplate object``` usually used for chat question answers \n",
    "    ```ChatPromptTemplate\n",
    "    HUMAN\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "- [flflo/rag-prompt](https://smith.langchain.com/hub/flflo/rag-prompt?organizationId=bd3fb686-7fb6-557c-95f9-8f1d8dcb7aee) is ```ChatPromptTemplate object``` for an assistant specialized teaching for a digital journalism course and it in french\n",
    "    ```HUMAN\n",
    "    Tu es un assistant pédagogique spécialisé pour un cours de journalisme numérique, avec la particularité que tous les échanges se déroulent en français. Ta mission est de répondre aux questions des étudiants sur les aspects techniques, théoriques et pratiques du journalisme numérique, en te référant uniquement aux documents du cours (présentations et notes). Cela inclut les sujets comme les tendances actuelles, l'analyse de données, et les études de cas en journalisme numérique. Si une question n'est pas claire, demande des précisions. Si la réponse ne se trouve pas dans les documents, indique simplement que tu ne sais pas, sans inventer de réponse. Respecte toujours la confidentialité et les données personnelles. Adapte tes réponses au niveau de compétence des étudiants et fournis des exemples et ressources supplémentaires si nécessaire. Encourage activement les étudiants à poser des questions pour faciliter un environnement d'apprentissage interactif et inclusif, tout en maintenant la communication en français.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "665e1d8a2f12a1ab"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "chattempobj = hub.pull(\"rlm/rag-prompt\")\n",
    "chattempobj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:15:47.160127600Z",
     "start_time": "2024-01-21T01:15:45.666279400Z"
    }
   },
   "id": "5c4849a753effd14",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\")])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.runnables.hub import HubRunnable # pip install langchainhub\n",
    "\n",
    "# Prompt is conf. with the rlm/rag-prompt prompt text\n",
    "prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n",
    "    owner_repo_commit=ConfigurableField(\n",
    "        id=\"hub_commit\",\n",
    "        name=\"Hub Commit\",\n",
    "        description=\"The Hub commit to pull from\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:28.621390300Z",
     "start_time": "2024-01-21T00:30:27.088610900Z"
    }
   },
   "id": "6972b9ce91f74151",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[HumanMessage(content=\"Tu es un assistant pédagogique spécialisé pour un cours de journalisme numérique, avec la particularité que tous les échanges se déroulent en français. Ta mission est de répondre aux questions des étudiants sur les aspects techniques, théoriques et pratiques du journalisme numérique, en te référant uniquement aux documents du cours (présentations et notes). Cela inclut les sujets comme les tendances actuelles, l'analyse de données, et les études de cas en journalisme numérique. Si une question n'est pas claire, demande des précisions. Si la réponse ne se trouve pas dans les documents, indique simplement que tu ne sais pas, sans inventer de réponse. Respecte toujours la confidentialité et les données personnelles. Adapte tes réponses au niveau de compétence des étudiants et fournis des exemples et ressources supplémentaires si nécessaire. Encourage activement les étudiants à poser des questions pour faciliter un environnement d'apprentissage interactif et inclusif, tout en maintenant la communication en français.\\nQuestion: foo \\nContext: bar \\nAnswer:\")])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prompt is reconf. with the rflflo/rag-prompt prompt text\n",
    "prompt.with_config(configurable={\"hub_commit\": \"flflo/rag-prompt\"}).invoke(\n",
    "    {\"question\": \"foo\", \"context\": \"bar\"}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:06:49.211990100Z",
     "start_time": "2024-01-21T01:06:48.633370900Z"
    }
   },
   "id": "9a8ff442263ec03e",
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c8f8ebfb40cbb7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configurable Alternatives"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a2ed826b955ded"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With LLMs (model Alt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6cd1c405193dbb4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatAnthropic(temperature=0).configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"anthropic\",\n",
    "    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n",
    "    openai=ChatOpenAI(),\n",
    "    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:29:18.441128500Z",
     "start_time": "2024-01-21T01:29:17.370273800Z"
    }
   },
   "id": "44abaf24088e4ade",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# By default it will call Anthropic but I do not have an Anthropic API Key\n",
    "# chain.invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:57:25.726850600Z",
     "start_time": "2024-01-21T01:57:25.670912600Z"
    }
   },
   "id": "785aa99da8486f66",
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conf. LLM as OpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771914de6ab16264"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use\n",
    "chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:30:20.699462200Z",
     "start_time": "2024-01-21T01:30:19.645259100Z"
    }
   },
   "id": "44c77dafbfdd25e1",
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61ef9f40837040d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With Prompts (conf alt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37df0fee1fd89613"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"joke\",\n",
    "    # This adds a new option, with name `poem`\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:43:34.868756300Z",
     "start_time": "2024-01-21T01:43:34.541134700Z"
    }
   },
   "id": "b0ae20d3bad937f4",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default it will write a joke\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:43:53.151763700Z",
     "start_time": "2024-01-21T01:43:52.016188200Z"
    }
   },
   "id": "687564c13e7c7fc8",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"In the wild, where nature thrives,\\nA creature roams with gentle strides.\\nWith fur so thick, and claws so strong,\\nThe mighty bear, where it belongs.\\n\\nIn forests deep, where shadows play,\\nThe bear emerges, night or day.\\nIts presence felt, a force untamed,\\nA symbol of strength, never maimed.\\n\\nWith eyes so wise, and heart so pure,\\nThe bear endures, forever sure.\\nThrough winters harsh, it finds its way,\\nIn solitude, it learns to sway.\\n\\nA guardian of the wild and free,\\nThe bear roams with dignity.\\nFrom mountains high to rivers wide,\\nIt teaches us to stand with pride.\\n\\nYet, in its might, a tender side,\\nA mother's love, it can't hide.\\nNurturing cubs, with gentle care,\\nA bond so strong, beyond compare.\\n\\nSo let us honor, this noble beast,\\nWith reverence, let our hearts feast.\\nFor in the bear, we find a grace,\\nA reminder of nature's embrace.\")"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can configure it write a poem\n",
    "chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:44:23.828135Z",
     "start_time": "2024-01-21T01:44:13.017312900Z"
    }
   },
   "id": "d8feb634a4cf47ea",
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9824206ee5baac36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With Prompts and LLMs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ef711c16b35e061"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(temperature=0).configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"anthropic\",\n",
    "    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`\n",
    "    openai=ChatOpenAI(),\n",
    "    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"joke\",\n",
    "    # This adds a new option, with name `poem`\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:55:05.599042200Z",
     "start_time": "2024-01-21T01:55:03.293765500Z"
    }
   },
   "id": "57d74adfe8f05b60",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"In forests deep, where sunlight peeks,\\nA mighty creature, strong and sleek,\\nWith fur of gold, and eyes so wise,\\nThe bear emerges, to our surprise.\\n\\nWith padded paws, it treads so light,\\nThrough mossy paths, in fading light,\\nA gentle giant, yet fierce and bold,\\nA story of wilderness, yet untold.\\n\\nIn hibernation, it seeks respite,\\nA slumber deep, through winter's night,\\nDreaming of rivers, and honey's sweet,\\nUntil spring arrives, with blossoms neat.\\n\\nThe bear, a symbol of strength and might,\\nProtector of nature, day and night,\\nWith each step it takes, a balance rare,\\nReminding us all, to handle with care.\\n\\nSo let us honor this majestic beast,\\nWith awe and reverence, let us feast,\\nFor in its presence, we are aware,\\nOf nature's wonders, beyond compare.\")"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can configure it write a poem with OpenAI\n",
    "chain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(\n",
    "    {\"topic\": \"bears\"}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:55:30.943798700Z",
     "start_time": "2024-01-21T01:55:21.346258600Z"
    }
   },
   "id": "38797443359aaab0",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can always just configure only one if we want\n",
    "chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:55:50.137784500Z",
     "start_time": "2024-01-21T01:55:49.017848300Z"
    }
   },
   "id": "fec93beec4a7e361",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6901077c4324a68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving configurations\n",
    "We can also easily save configured chains as their own objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f12e522c30b2c07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "openai_poem = chain.with_config(configurable={\"llm\": \"openai\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:56:54.487807700Z",
     "start_time": "2024-01-21T01:56:54.431083100Z"
    }
   },
   "id": "f35340d73bbc2029",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_poem.invoke({\"topic\": \"bears\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T01:57:04.376970700Z",
     "start_time": "2024-01-21T01:57:03.191714200Z"
    }
   },
   "id": "3735e8862479a7ad",
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca8ee1e0a74ed6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8904b081cd654e1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
